{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVVXcplghy0k",
        "outputId": "dec81985-4ec9-4e6c-f251-803970ac597b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Silent Impact Rankings (Scaled to 100):\n",
            "\n",
            "carolonsloe: {'helped_others': 0.5, 'received_public_praise': 0, 'unacknowledged_contribs': 0, 'proactive_support': 1, 'total_score': 100.0}\n",
            "aishakhan: {'helped_others': 0, 'received_public_praise': 0, 'unacknowledged_contribs': 1, 'proactive_support': 0, 'total_score': 88.89}\n",
            "bobmarley: {'helped_others': 1, 'received_public_praise': 0, 'unacknowledged_contribs': 0, 'proactive_support': 0, 'total_score': 66.67}\n",
            "frankfruit: {'helped_others': 1, 'received_public_praise': 0, 'unacknowledged_contribs': 0, 'proactive_support': 0, 'total_score': 66.67}\n",
            "ianoneill: {'helped_others': 0, 'received_public_praise': 0, 'unacknowledged_contribs': 0, 'proactive_support': 1, 'total_score': 66.67}\n",
            "helenkeller: {'helped_others': 0.5, 'received_public_praise': 0, 'unacknowledged_contribs': 0, 'proactive_support': 0, 'total_score': 33.33}\n"
          ]
        }
      ],
      "source": [
        "from collections import defaultdict\n",
        "import re\n",
        "\n",
        "# our employees\n",
        "employees = [\"ianoneill\", \"helenkeller\", \"frankfruit\", \"carolonsloe\", \"bobmarley\", \"aishakhan\"]\n",
        "\n",
        "#mocked GitHub PR Data\n",
        "github_data = [\n",
        "    {\n",
        "      \"pull_request\": {\n",
        "        \"title\": \"Add automated tests for auth service\",\n",
        "        \"author\": \"ianoneill\",\n",
        "        \"description\": \"Added 10+ test cases to increase coverage by 22%\",\n",
        "        \"reviewers\": [\n",
        "          {\n",
        "            \"reviewer\": \"bobmarley\",\n",
        "            \"comment\": \"This is fantastic coverage. Really nice job!\",\n",
        "            \"sentiment\": \"positive\"\n",
        "          },\n",
        "          {\n",
        "            \"reviewer\": \"helenkeller\",\n",
        "            \"comment\": \"Left a few minor comments, but overall LGTM.\",\n",
        "            \"sentiment\": \"neutral\"\n",
        "          }\n",
        "        ],\n",
        "        \"merged_by\": \"carolonsloe\",\n",
        "        \"acknowledged_contributor\": \"none\"\n",
        "      }\n",
        "    },\n",
        "    {\n",
        "      \"pull_request\": {\n",
        "        \"title\": \"Fix flaky dashboard rendering bug\",\n",
        "        \"author\": \"helenkeller\",\n",
        "        \"description\": \"Resolved intermittent loading issue on dashboard page.\",\n",
        "        \"reviewers\": [\n",
        "          {\n",
        "            \"reviewer\": \"frankfruit\",\n",
        "            \"comment\": \"Thanks for fixing this â€” I hit this bug twice last week!\",\n",
        "            \"sentiment\": \"positive\"\n",
        "          }\n",
        "        ],\n",
        "        \"merged_by\": \"ianoneill\",\n",
        "        \"acknowledged_contributor\": \"none\"\n",
        "      }\n",
        "    },\n",
        "    {\n",
        "      \"pull_request\": {\n",
        "        \"title\": \"Update analytics pipeline for new metrics\",\n",
        "        \"author\": \"bobmarley\",\n",
        "        \"description\": \"Ingests new usage events and feeds them into the existing funnel structure.\",\n",
        "        \"reviewers\": [\n",
        "          {\n",
        "            \"reviewer\": \"carolonsloe\",\n",
        "            \"comment\": \"Big change â€” I left a few questions but overall looks good.\",\n",
        "            \"sentiment\": \"neutral\"\n",
        "          }\n",
        "        ],\n",
        "        \"merged_by\": \"frankfruit\",\n",
        "        \"acknowledged_contributor\": \"aishakhan\"\n",
        "      }\n",
        "    }\n",
        "]\n",
        "\n",
        "#transcript Data\n",
        "transcript = [\n",
        "    { \"speaker\": \"ianoneill\", \"timestamp\": \"00:00:01\", \"text\": \"Morning, team! Quick sync today â€“ mostly updates.\" },\n",
        "    { \"speaker\": \"carolonsloe\", \"timestamp\": \"00:00:05\", \"text\": \"Weâ€™ve been blocked on the API test cases again.\" },\n",
        "    { \"speaker\": \"bobmarley\", \"timestamp\": \"00:00:09\", \"text\": \"Iâ€™ll jump on that after this, Mark.\" },\n",
        "    { \"speaker\": \"ianoneill\", \"timestamp\": \"00:00:12\", \"text\": \"Thanks Bob, shoutout to Helen for the docs!\" },\n",
        "    { \"speaker\": \"helenkeller\", \"timestamp\": \"00:00:16\", \"text\": \"No problem â€” happy to help!\" },\n",
        "    { \"speaker\": \"carolonsloe\", \"timestamp\": \"00:00:20\", \"text\": \"Thanks Helen! And a huge shoutout to Ian for pushing that PR through.\" },\n",
        "    { \"speaker\": \"frankfruit\", \"timestamp\": \"00:00:24\", \"text\": \"Great job, everyone. Let's keep this momentum going.\" },\n",
        "    { \"speaker\": \"aishakhan\", \"timestamp\": \"00:00:28\", \"text\": \"Iâ€™ve been reviewing the analytics changes â€” looking solid so far.\" },\n",
        "]\n",
        "\n",
        "#Teams Chat Data\n",
        "teams_chat = [\n",
        "    {\n",
        "      \"sender\": \"bobmarley\",\n",
        "      \"timestamp\": \"2025-04-08T13:20:00Z\",\n",
        "      \"text\": \"Anyone have experience with the new Azure deployment config? Seeing weird errors.\"\n",
        "    },\n",
        "    {\n",
        "      \"sender\": \"ianoneill\",\n",
        "      \"timestamp\": \"2025-04-08T13:21:30Z\",\n",
        "      \"text\": \"Yeah, I ran into this last week. Try setting `enableDefaultIngress` to true.\"\n",
        "    },\n",
        "    {\n",
        "      \"sender\": \"helenkeller\",\n",
        "      \"timestamp\": \"2025-04-08T13:23:10Z\",\n",
        "      \"text\": \"That did the trick. Thanks, Ian!\"\n",
        "    },\n",
        "    {\n",
        "      \"sender\": \"frankfruit\",\n",
        "      \"timestamp\": \"2025-04-09T09:15:00Z\",\n",
        "      \"text\": \"Need someone to sanity-check this SQL migration before I run it.\"\n",
        "    },\n",
        "    {\n",
        "      \"sender\": \"carolonsloe\",\n",
        "      \"timestamp\": \"2025-04-09T09:17:00Z\",\n",
        "      \"text\": \"On it. Looking now.\"\n",
        "    },\n",
        "    {\n",
        "      \"sender\": \"bobmarley\",\n",
        "      \"timestamp\": \"2025-04-09T09:22:00Z\",\n",
        "      \"text\": \"Looks clean, just double-check the rollback logic.\"\n",
        "    },\n",
        "    {\n",
        "      \"sender\": \"aishakhan\",\n",
        "      \"timestamp\": \"2025-04-10T11:00:00Z\",\n",
        "      \"text\": \"Huge thanks to Ian for unblocking the deployment flow. Quietly crushed it again ğŸ‘\"\n",
        "    }\n",
        "]\n",
        "\n",
        "#initialize scores for the six employees\n",
        "impact_scores = defaultdict(lambda: {\n",
        "    \"helped_others\": 0,\n",
        "    \"received_public_praise\": 0,\n",
        "    \"unacknowledged_contribs\": 0,\n",
        "    \"proactive_support\": 0,\n",
        "    \"total_score\": 0\n",
        "})\n",
        "\n",
        "# ---------- GITHUB DATA ANALYSIS ----------\n",
        "for pr in github_data:\n",
        "    pr_data = pr[\"pull_request\"]\n",
        "    author = pr_data[\"author\"]\n",
        "    reviewers = pr_data[\"reviewers\"]\n",
        "    acknowledged = pr_data.get(\"acknowledged_contributor\")\n",
        "\n",
        "    # PR authorship is a base contribution â€” not silent unless ignored\n",
        "    # If acknowledged contributor is present and not author, give score\n",
        "    if acknowledged and acknowledged != author:\n",
        "        if acknowledged != \"none\":\n",
        "            impact_scores[acknowledged][\"unacknowledged_contribs\"] += 1\n",
        "\n",
        "    for review in reviewers:\n",
        "        sentiment = review[\"sentiment\"]\n",
        "        reviewer = review[\"reviewer\"]\n",
        "        if sentiment == \"positive\":\n",
        "            impact_scores[reviewer][\"helped_others\"] += 1\n",
        "        elif sentiment == \"neutral\":\n",
        "            impact_scores[reviewer][\"helped_others\"] += 0.5\n",
        "\n",
        "# ---------- TRANSCRIPT ANALYSIS ----------\n",
        "for entry in transcript:\n",
        "    text = entry[\"text\"]\n",
        "    speaker = entry[\"speaker\"]\n",
        "\n",
        "    #look for praise or mentions by others\n",
        "    for person in employees:\n",
        "        if person != speaker and re.search(rf\"\\b{person}\\b\", text, re.IGNORECASE):\n",
        "            if any(word in text.lower() for word in [\"thanks\", \"shoutout\", \"helped\", \"fix\", \"great job\", \"nice work\", \"awesome\"]):\n",
        "                impact_scores[person][\"received_public_praise\"] += 1\n",
        "\n",
        "# ---------- TEAMS CHAT ANALYSIS ----------\n",
        "for message in teams_chat:\n",
        "    sender = message[\"sender\"]\n",
        "    text = message[\"text\"]\n",
        "\n",
        "    for person in employees:\n",
        "        if person != sender and re.search(rf\"\\b{person}\\b\", text, re.IGNORECASE):\n",
        "            if \"thanks\" in text.lower() or \"appreciate\" in text.lower():\n",
        "                impact_scores[person][\"received_public_praise\"] += 1\n",
        "\n",
        "    #look for helpful replies (technical advice, support)\n",
        "    if any(word in text.lower() for word in [\"try\", \"suggest\", \"set\", \"fix\", \"looking\", \"checked\", \"reviewed\"]):\n",
        "        impact_scores[sender][\"proactive_support\"] += 1\n",
        "\n",
        "# ---------- FINAL SCORING ----------\n",
        "max_score = 0  # We'll find the maximum score for scaling\n",
        "\n",
        "for person, metrics in impact_scores.items(): #weighting each category\n",
        "    score = (\n",
        "        1.5 * metrics[\"helped_others\"] +\n",
        "        2.0 * metrics[\"unacknowledged_contribs\"] +\n",
        "        1.2 * metrics[\"received_public_praise\"] +\n",
        "        1.5 * metrics[\"proactive_support\"]\n",
        "    )\n",
        "    metrics[\"total_score\"] = round(score, 2)\n",
        "    max_score = max(max_score, metrics[\"total_score\"])  # Keep track of the highest score\n",
        "\n",
        "#scale scores to 100\n",
        "for person, metrics in impact_scores.items():\n",
        "    metrics[\"total_score\"] = round((metrics[\"total_score\"] / max_score) * 100, 2)\n",
        "\n",
        "# ---------- OUTPUT RESULTS ----------\n",
        "sorted_impact = sorted(impact_scores.items(), key=lambda x: x[1]['total_score'], reverse=True)\n",
        "\n",
        "print(\"Silent Impact Rankings (Scaled to 100):\\n\")\n",
        "for person, metrics in sorted_impact:\n",
        "    print(f\"{person}: {metrics}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers torch\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DwVRuBiAix7P",
        "outputId": "4f0cebb6-bded-42a7-8502-7794645ddd0c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m127.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m99.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m55.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m108.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ocSIA93mjCTg",
        "outputId": "e1eb5bb7-4595-4285-afd6-e40132dfd4ad"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to get recent work for each employee\n",
        "def get_recent_work(name, transcript, teams_chat, github_data):\n",
        "    work = []\n",
        "\n",
        "    # GitHub PRs\n",
        "    for pr in github_data:\n",
        "        pr_data = pr[\"pull_request\"]\n",
        "        if pr_data[\"author\"] == name:\n",
        "            work.append(f\"{name} authored PR: {pr_data['title']} - {pr_data['description']}\")\n",
        "\n",
        "    # Transcript mentions\n",
        "    for entry in transcript:\n",
        "        if entry[\"speaker\"] == name or name in entry[\"text\"]:\n",
        "            work.append(f\"{name} said: {entry['text']}\")\n",
        "\n",
        "    # Teams chat mentions\n",
        "    for message in teams_chat:\n",
        "        if message[\"sender\"] == name or name in message[\"text\"]:\n",
        "            work.append(f\"{name} in Teams: {message['text']}\")\n",
        "\n",
        "    return work\n",
        "\n",
        "# Function to get AI Insights for each employee\n",
        "def get_ai_insights(name, transcript, teams_chat, github_data):\n",
        "    recent_work = get_recent_work(name, transcript, teams_chat, github_data)\n",
        "    full_text = f\"Recent work for {name}:\\n\" + \"\\n\".join(recent_work)\n",
        "\n",
        "    # Generate insights using the summarizer\n",
        "    insights = summarizer(full_text, max_length=50, min_length=20, do_sample=False)\n",
        "    return insights[0]['summary_text']\n",
        "\n",
        "# Get and print the results for each employee\n",
        "for employee in employees:\n",
        "    recent_work = get_recent_work(employee, transcript, teams_chat, github_data)\n",
        "    ai_insights = get_ai_insights(employee, transcript, teams_chat, github_data)\n",
        "\n",
        "    print(f\"\\nRecent Work for {employee}:\\n\")\n",
        "    for work in recent_work:\n",
        "        print(f\"- {work}\")\n",
        "\n",
        "    print(f\"\\nAI Insights for {employee}:\\n{ai_insights}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ur3rTMBKnwxX",
        "outputId": "f6a48244-9bbd-4b8d-eb1b-68b6d01beb6d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Recent Work for ianoneill:\n",
            "\n",
            "- ianoneill authored PR: Add automated tests for auth service - Added 10+ test cases to increase coverage by 22%\n",
            "- ianoneill said: Morning, team! Quick sync today â€“ mostly updates.\n",
            "- ianoneill said: Thanks Bob, shoutout to Helen for the docs!\n",
            "- ianoneill in Teams: Yeah, I ran into this last week. Try setting `enableDefaultIngress` to true.\n",
            "\n",
            "AI Insights for ianoneill:\n",
            "ianoneill said: Thanks Bob, shoutout to Helen for the docs! Try setting `enableDefaultIngress` to true.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Your max_length is set to 50, but your input_length is only 47. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=23)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Recent Work for helenkeller:\n",
            "\n",
            "- helenkeller authored PR: Fix flaky dashboard rendering bug - Resolved intermittent loading issue on dashboard page.\n",
            "- helenkeller said: No problem â€” happy to help!\n",
            "- helenkeller in Teams: That did the trick. Thanks, Ian!\n",
            "\n",
            "AI Insights for helenkeller:\n",
            "Helenkeller said: No problem â€” happy to help! Ian said: That did the trick. Thanks, Ian!\n",
            "\n",
            "Recent Work for frankfruit:\n",
            "\n",
            "- frankfruit said: Great job, everyone. Let's keep this momentum going.\n",
            "- frankfruit in Teams: Need someone to sanity-check this SQL migration before I run it.\n",
            "\n",
            "AI Insights for frankfruit:\n",
            "frankfruit said: Great job, everyone. Let's keep this momentum going. Need someone to sanity-check this SQL migration before I run it.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Recent Work for carolonsloe:\n",
            "\n",
            "- carolonsloe said: Weâ€™ve been blocked on the API test cases again.\n",
            "- carolonsloe said: Thanks Helen! And a huge shoutout to Ian for pushing that PR through.\n",
            "- carolonsloe in Teams: On it. Looking now.\n",
            "\n",
            "AI Insights for carolonsloe:\n",
            "Carolonsloe said: Weâ€™ve been blocked on the API test cases again.\n",
            "\n",
            "Recent Work for bobmarley:\n",
            "\n",
            "- bobmarley authored PR: Update analytics pipeline for new metrics - Ingests new usage events and feeds them into the existing funnel structure.\n",
            "- bobmarley said: Iâ€™ll jump on that after this, Mark.\n",
            "- bobmarley in Teams: Anyone have experience with the new Azure deployment config? Seeing weird errors.\n",
            "- bobmarley in Teams: Looks clean, just double-check the rollback logic.\n",
            "\n",
            "AI Insights for bobmarley:\n",
            "bobmarley authored PR: Update analytics pipeline for new metrics. Ingests new usage events and feeds them into the existing funnel structure.\n",
            "\n",
            "Recent Work for aishakhan:\n",
            "\n",
            "- aishakhan said: Iâ€™ve been reviewing the analytics changes â€” looking solid so far.\n",
            "- aishakhan in Teams: Huge thanks to Ian for unblocking the deployment flow. Quietly crushed it again ğŸ‘\n",
            "\n",
            "AI Insights for aishakhan:\n",
            "aishakhan: Iâ€™ve been reviewing the analytics changes â€” looking solid so far. Huge thanks to Ian for unblocking the deployment flow. Quietly crushed it again.\n"
          ]
        }
      ]
    }
  ]
}